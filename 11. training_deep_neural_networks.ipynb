{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3641ee1",
   "metadata": {},
   "source": [
    "# 11장 심층 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3997a",
   "metadata": {},
   "source": [
    "## 실용적인 가이드라인\n",
    "- 커널 초기화: He 초기화\n",
    "- 활성화 함수: ELU 혹은 빠른 속도의 ReLU\n",
    "- 정규화: 얕은 신경망일 경우 벗음. 깊다면 Batch Normalization\n",
    "- 규제: 조기 종료 (필요하면 l2 규제)\n",
    "- 옵티마이저: Adam (또는 RMSProp이나 Nadam)\n",
    "- LR 스케쥴: 1사이클"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83de0c",
   "metadata": {},
   "source": [
    "## 연습문제 8\n",
    "CIFAR10 이미지 데이터셋에 심층 신경망을 훈련해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5569eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b27622",
   "metadata": {},
   "source": [
    "a.문제: 100개의 뉴런을 가진 은닉층 20개로 심층 신경망을 만들어보세요(너무 많은 것 같지만 이 연습문제의 핵심입니다). He 초기화와 ELU 활성화 함수를 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477936e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd0b97",
   "metadata": {},
   "source": [
    "b.문제: Nadam 옵티마이저와 조기 종료를 사용하여 CIFAR10 데이터셋에 이 네트워크를 훈련하세요. keras.datasets.cifar10.load_ data()를 사용하여 데이터를 적재할 수 있습니다. 이 데이터셋은 10개의 클래스와 32×32 크기의 컬러 이미지 60,000개로 구성됩니다(50,000개는 훈련, 10,000개는 테스트). 따라서 10개의 뉴런과 소프트맥스 활성화 함수를 사용하는 출력층이 필요합니다. 모델 구조와 하이퍼파라미터를 바꿀 때마다 적절한 학습률을 찾아야 한다는 것을 기억하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39251c0c",
   "metadata": {},
   "source": [
    "출력층 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f8c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7ac1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anpdi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6035111",
   "metadata": {},
   "source": [
    "CIFAR10 데이터셋을 로드하죠. 조기 종료를 사용하기 때문에 검증 세트가 필요합니다. 원본 훈련 세트에서 처음 5,000개를 검증 세트로 사용하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07299a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 29s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "342af505",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # 모델을 훈련할 때마다 증가시킴\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbf9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6c8685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 13s 7ms/step - loss: 4.0526 - accuracy: 0.1732 - val_loss: 2.1033 - val_accuracy: 0.2440\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 2.0263 - accuracy: 0.2588 - val_loss: 1.9987 - val_accuracy: 0.2574\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.9162 - accuracy: 0.2988 - val_loss: 2.0941 - val_accuracy: 0.2452\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.8484 - accuracy: 0.3247 - val_loss: 1.9739 - val_accuracy: 0.3034\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7965 - accuracy: 0.3454 - val_loss: 1.7953 - val_accuracy: 0.3454\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.7432 - accuracy: 0.3687 - val_loss: 1.7447 - val_accuracy: 0.3640\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7041 - accuracy: 0.3830 - val_loss: 1.7180 - val_accuracy: 0.3770\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6685 - accuracy: 0.3959 - val_loss: 1.6864 - val_accuracy: 0.3944\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6371 - accuracy: 0.4094 - val_loss: 1.6482 - val_accuracy: 0.4086\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6127 - accuracy: 0.4178 - val_loss: 1.6771 - val_accuracy: 0.3894\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5891 - accuracy: 0.4265 - val_loss: 1.6898 - val_accuracy: 0.3938\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5645 - accuracy: 0.4367 - val_loss: 1.6563 - val_accuracy: 0.4064\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5455 - accuracy: 0.4426 - val_loss: 1.6263 - val_accuracy: 0.4126\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5278 - accuracy: 0.4510 - val_loss: 1.5947 - val_accuracy: 0.4272\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5104 - accuracy: 0.4566 - val_loss: 1.5662 - val_accuracy: 0.4420\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4935 - accuracy: 0.4623 - val_loss: 1.5663 - val_accuracy: 0.4460\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4792 - accuracy: 0.4714 - val_loss: 1.5550 - val_accuracy: 0.4418\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4629 - accuracy: 0.4759 - val_loss: 1.5579 - val_accuracy: 0.4452\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4493 - accuracy: 0.4827 - val_loss: 1.5573 - val_accuracy: 0.4490\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4393 - accuracy: 0.4841 - val_loss: 1.5517 - val_accuracy: 0.4468\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4237 - accuracy: 0.4909 - val_loss: 1.5850 - val_accuracy: 0.4408\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4125 - accuracy: 0.4946 - val_loss: 1.5542 - val_accuracy: 0.4506\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4002 - accuracy: 0.4977 - val_loss: 1.5516 - val_accuracy: 0.4356\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3872 - accuracy: 0.5045 - val_loss: 1.5160 - val_accuracy: 0.4612\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3724 - accuracy: 0.5085 - val_loss: 1.5244 - val_accuracy: 0.4670\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3636 - accuracy: 0.5105 - val_loss: 1.5425 - val_accuracy: 0.4526\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3556 - accuracy: 0.5175 - val_loss: 1.5056 - val_accuracy: 0.4646\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3450 - accuracy: 0.5181 - val_loss: 1.5204 - val_accuracy: 0.4628\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3383 - accuracy: 0.5228 - val_loss: 1.5178 - val_accuracy: 0.4608\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3255 - accuracy: 0.5278 - val_loss: 1.5295 - val_accuracy: 0.4586\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3200 - accuracy: 0.5266 - val_loss: 1.5299 - val_accuracy: 0.4694\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3068 - accuracy: 0.5331 - val_loss: 1.5340 - val_accuracy: 0.4644\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2992 - accuracy: 0.5365 - val_loss: 1.5327 - val_accuracy: 0.4558\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2919 - accuracy: 0.5389 - val_loss: 1.5236 - val_accuracy: 0.4646\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2817 - accuracy: 0.5407 - val_loss: 1.5200 - val_accuracy: 0.4654\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2736 - accuracy: 0.5450 - val_loss: 1.5159 - val_accuracy: 0.4640\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2631 - accuracy: 0.5491 - val_loss: 1.5428 - val_accuracy: 0.4720\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2556 - accuracy: 0.5511 - val_loss: 1.5043 - val_accuracy: 0.4672\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2503 - accuracy: 0.5538 - val_loss: 1.5315 - val_accuracy: 0.4624\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2368 - accuracy: 0.5575 - val_loss: 1.5217 - val_accuracy: 0.4658\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2325 - accuracy: 0.5600 - val_loss: 1.5160 - val_accuracy: 0.4726\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2248 - accuracy: 0.5625 - val_loss: 1.5689 - val_accuracy: 0.4602\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2174 - accuracy: 0.5643 - val_loss: 1.5339 - val_accuracy: 0.4702\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2070 - accuracy: 0.5707 - val_loss: 1.5713 - val_accuracy: 0.4702\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2008 - accuracy: 0.5715 - val_loss: 1.5143 - val_accuracy: 0.4722\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1948 - accuracy: 0.5728 - val_loss: 1.5264 - val_accuracy: 0.4770\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1874 - accuracy: 0.5739 - val_loss: 1.5404 - val_accuracy: 0.4724\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1797 - accuracy: 0.5794 - val_loss: 1.5423 - val_accuracy: 0.4818\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1709 - accuracy: 0.5820 - val_loss: 1.5390 - val_accuracy: 0.4628\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1646 - accuracy: 0.5846 - val_loss: 1.5667 - val_accuracy: 0.4696\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1595 - accuracy: 0.5863 - val_loss: 1.5464 - val_accuracy: 0.4708\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1487 - accuracy: 0.5899 - val_loss: 1.5715 - val_accuracy: 0.4662\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1422 - accuracy: 0.5912 - val_loss: 1.5487 - val_accuracy: 0.4730\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1390 - accuracy: 0.5942 - val_loss: 1.5662 - val_accuracy: 0.4712\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1315 - accuracy: 0.5947 - val_loss: 1.5527 - val_accuracy: 0.4724\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1250 - accuracy: 0.5984 - val_loss: 1.5507 - val_accuracy: 0.4722\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1138 - accuracy: 0.6020 - val_loss: 1.5707 - val_accuracy: 0.4700\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1111 - accuracy: 0.6036 - val_loss: 1.5477 - val_accuracy: 0.4706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e789a48760>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96474fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5043 - accuracy: 0.4672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5042527914047241, 0.46720001101493835]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20521446",
   "metadata": {},
   "source": [
    "c.문제: 배치 정규화를 추가하고 학습 곡선을 비교해보세요. 이전보다 빠르게 수렴하나요? 더 좋은 모델이 만들어지나요? 훈련 속도에는 어떤 영향을 미치나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f022fce",
   "metadata": {},
   "source": [
    "- 출력층을 제외하고 모든 Dense 층 다음에 (활성화 함수 전에) BN 층을 추가했습니다. 처음 은닉층 전에도 BN 층을 추가했습니다.\n",
    "- 학습률을 5e-4로 바꾸었습니다. 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3, 3e-3를 시도해 보고 20번 에포크 후에 검증 세트 성능이 가장 좋은 것을 선택했습니다.\n",
    "- run_logdir를 runbn* 으로 이름을 바꾸고 모델 파일 이름을 my_cifar10_bn_model.h5로 변경했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc03165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 30s 17ms/step - loss: 1.8440 - accuracy: 0.3392 - val_loss: 1.7015 - val_accuracy: 0.3874\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6689 - accuracy: 0.4068 - val_loss: 1.6215 - val_accuracy: 0.4134\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5991 - accuracy: 0.4308 - val_loss: 1.5559 - val_accuracy: 0.4470\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5487 - accuracy: 0.4485 - val_loss: 1.5148 - val_accuracy: 0.4586\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5077 - accuracy: 0.4622 - val_loss: 1.4376 - val_accuracy: 0.4878\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4674 - accuracy: 0.4796 - val_loss: 1.4259 - val_accuracy: 0.4906\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4378 - accuracy: 0.4880 - val_loss: 1.4197 - val_accuracy: 0.4998\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4069 - accuracy: 0.5014 - val_loss: 1.4065 - val_accuracy: 0.4986\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3879 - accuracy: 0.5081 - val_loss: 1.3778 - val_accuracy: 0.5146\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3660 - accuracy: 0.5159 - val_loss: 1.3635 - val_accuracy: 0.5168\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3452 - accuracy: 0.5206 - val_loss: 1.3624 - val_accuracy: 0.5156\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3248 - accuracy: 0.5312 - val_loss: 1.3855 - val_accuracy: 0.5022\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3038 - accuracy: 0.5364 - val_loss: 1.3661 - val_accuracy: 0.5200\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2874 - accuracy: 0.5428 - val_loss: 1.3506 - val_accuracy: 0.5252\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2676 - accuracy: 0.5521 - val_loss: 1.3734 - val_accuracy: 0.5172\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2590 - accuracy: 0.5526 - val_loss: 1.3591 - val_accuracy: 0.5244\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2408 - accuracy: 0.5628 - val_loss: 1.3222 - val_accuracy: 0.5354\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2242 - accuracy: 0.5677 - val_loss: 1.3334 - val_accuracy: 0.5346\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2107 - accuracy: 0.5720 - val_loss: 1.3433 - val_accuracy: 0.5338\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1984 - accuracy: 0.5767 - val_loss: 1.3801 - val_accuracy: 0.5176\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1859 - accuracy: 0.5812 - val_loss: 1.3662 - val_accuracy: 0.5186\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1719 - accuracy: 0.5855 - val_loss: 1.3340 - val_accuracy: 0.5338\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1617 - accuracy: 0.5914 - val_loss: 1.3232 - val_accuracy: 0.5348\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1453 - accuracy: 0.5965 - val_loss: 1.3235 - val_accuracy: 0.5398\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1384 - accuracy: 0.5976 - val_loss: 1.3431 - val_accuracy: 0.5330\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1219 - accuracy: 0.6046 - val_loss: 1.3313 - val_accuracy: 0.5408\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1098 - accuracy: 0.6092 - val_loss: 1.3564 - val_accuracy: 0.5340\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1054 - accuracy: 0.6124 - val_loss: 1.3605 - val_accuracy: 0.5346\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0948 - accuracy: 0.6136 - val_loss: 1.3308 - val_accuracy: 0.5386\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0807 - accuracy: 0.6172 - val_loss: 1.3327 - val_accuracy: 0.5380\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0716 - accuracy: 0.6195 - val_loss: 1.3461 - val_accuracy: 0.5426\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0622 - accuracy: 0.6254 - val_loss: 1.3536 - val_accuracy: 0.5390\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0466 - accuracy: 0.6303 - val_loss: 1.3537 - val_accuracy: 0.5502\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0450 - accuracy: 0.6320 - val_loss: 1.3397 - val_accuracy: 0.5468\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0323 - accuracy: 0.6356 - val_loss: 1.3870 - val_accuracy: 0.5296\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0216 - accuracy: 0.6391 - val_loss: 1.3618 - val_accuracy: 0.5386\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0122 - accuracy: 0.6411 - val_loss: 1.3514 - val_accuracy: 0.5398\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 1.3222 - accuracy: 0.5354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3221758604049683, 0.5353999733924866]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # 모델을 훈련할 때마다 증가시킴\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaeb432",
   "metadata": {},
   "source": [
    "- 이전보다 빠르게 수렴하나요? 훨씬 빠릅니다! 이전 모델은 가장 낮은 검증 손실에 도달하기 위해 27 에포크가 걸렸지만 새 모델은 동일한 손실에 도달하는데 5 에포크가 걸렸고 16 에포크까지 계속 줄어듭니다. 이전 모델보다 두 배 이상 빠릅니다. BN 층은 훈련을 안정적으로 수행하고 더 큰 학습률을 사용할 수 있기 때문에 수렴이 빨라졌습니다.\n",
    "- BN이 더 좋은 모델을 만드나요? 네! 최종 모델의 성능이 47.6%가 아니라 54.0% 정확도로 더 좋습니다. 이는 아주 좋은 모델이 아니지만 적어도 이전보다는 낫습니다(합성곱 신경망이 더 낫겠지만 이는 다른 주제입니다. 14장을 참고하세요).\n",
    "- BN이 훈련 속도에 영향을 미치나요? 모델이 훨씬 빠르게 수렴했지만 각 에포크는 8초가 아니라 12초가 걸렸습니다. BN 층에서 추가된 계산 때문입니다. 하지만 전반적인 훈련 시간(탁상 시계 시간)은 크게 줄었습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d4e86",
   "metadata": {},
   "source": [
    "d.문제: 배치 정규화를 SELU로 바꾸어보세요. 네트워크가 자기 정규화하기 위해 필요한 변경 사항을 적용해보세요(즉, 입력 특성 표준화, 르쿤 정규분포 초기화, 완전 연결 층만 순차적으로 쌓은 심층 신경망 등)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be0141b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkeras.backend.clear_session()\\ntf.random.set_seed(42)\\nnp.random.seed(42)\\n\\nmodel = keras.models.Sequential()\\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\\nfor _ in range(20):\\n    model.add(keras.layers.Dense(100,\\n                                 kernel_initializer=\"lecun_normal\",\\n                                 activation=\"selu\"))\\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\\n\\noptimizer = keras.optimizers.Nadam(lr=7e-4)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\\n              optimizer=optimizer,\\n              metrics=[\"accuracy\"])\\n\\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\\nrun_index = 1 # 모델을 훈련할 때마다 증가시킴\\nrun_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\\ncallbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\\n\\nX_means = X_train.mean(axis=0)\\nX_stds = X_train.std(axis=0)\\nX_train_scaled = (X_train - X_means) / X_stds\\nX_valid_scaled = (X_valid - X_means) / X_stds\\nX_test_scaled = (X_test - X_means) / X_stds\\n\\nmodel.fit(X_train_scaled, y_train, epochs=100,\\n          validation_data=(X_valid_scaled, y_valid),\\n          callbacks=callbacks)\\n\\nmodel = keras.models.load_model(\"my_cifar10_selu_model.h5\")\\nmodel.evaluate(X_valid_scaled, y_valid)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # 모델을 훈련할 때마다 증가시킴\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade0984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "#model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3711dff",
   "metadata": {},
   "source": [
    "47.9% 정확도를 얻었습니다. 원래 모델(47.6%)보다 크게 높지 않습니다. 배치 정규화를 사용한 모델(54.0%)만큼 좋지도 않습니다. 하지만 BN 모델만큼 빠르게 수렴했습니다. 각 에포크는 7초만 걸렸습니다. 따라서 이 모델이 지금까지 가장 빠른 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee85a56",
   "metadata": {},
   "source": [
    "e.문제: 알파 드롭아웃으로 모델에 규제를 적용해보세요. 그다음 모델을 다시 훈련하지 않고 MC 드롭아웃으로 더 높은 정확도를 얻을 수 있는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9488e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 1.8898 - accuracy: 0.3311 - val_loss: 1.7277 - val_accuracy: 0.3878\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.6642 - accuracy: 0.4141 - val_loss: 1.6905 - val_accuracy: 0.3818\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5691 - accuracy: 0.4506 - val_loss: 1.6987 - val_accuracy: 0.4296\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5048 - accuracy: 0.4692 - val_loss: 1.5738 - val_accuracy: 0.4618\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4495 - accuracy: 0.4944 - val_loss: 1.6148 - val_accuracy: 0.4488\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4038 - accuracy: 0.5096 - val_loss: 1.5209 - val_accuracy: 0.4866\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3614 - accuracy: 0.5250 - val_loss: 1.5578 - val_accuracy: 0.4888\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3219 - accuracy: 0.5417 - val_loss: 1.4751 - val_accuracy: 0.5062\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2885 - accuracy: 0.5509 - val_loss: 1.4847 - val_accuracy: 0.4956\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 1.2537 - accuracy: 0.5638 - val_loss: 1.5022 - val_accuracy: 0.4958\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2178 - accuracy: 0.5794 - val_loss: 1.5336 - val_accuracy: 0.5030\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1969 - accuracy: 0.5857 - val_loss: 1.5276 - val_accuracy: 0.4946\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1648 - accuracy: 0.5971 - val_loss: 1.5747 - val_accuracy: 0.5084\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1377 - accuracy: 0.6062 - val_loss: 1.5851 - val_accuracy: 0.5020\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1149 - accuracy: 0.6170 - val_loss: 1.5829 - val_accuracy: 0.5160\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0857 - accuracy: 0.6267 - val_loss: 1.6661 - val_accuracy: 0.5016\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0621 - accuracy: 0.6370 - val_loss: 1.6201 - val_accuracy: 0.5146\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0439 - accuracy: 0.6419 - val_loss: 1.6137 - val_accuracy: 0.5178\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0275 - accuracy: 0.6483 - val_loss: 1.6822 - val_accuracy: 0.5142\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0015 - accuracy: 0.6561 - val_loss: 1.6809 - val_accuracy: 0.5172\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9768 - accuracy: 0.6679 - val_loss: 1.6911 - val_accuracy: 0.5078\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9601 - accuracy: 0.6714 - val_loss: 1.7258 - val_accuracy: 0.5172\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9341 - accuracy: 0.6800 - val_loss: 1.7753 - val_accuracy: 0.5038\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9246 - accuracy: 0.6838 - val_loss: 1.6938 - val_accuracy: 0.5030\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9045 - accuracy: 0.6882 - val_loss: 1.8045 - val_accuracy: 0.5046\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8850 - accuracy: 0.6978 - val_loss: 1.8794 - val_accuracy: 0.5102\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8753 - accuracy: 0.7008 - val_loss: 1.7908 - val_accuracy: 0.5100\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8634 - accuracy: 0.7061 - val_loss: 1.7940 - val_accuracy: 0.4974\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 1.4751 - accuracy: 0.5062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4750503301620483, 0.5062000155448914]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # 모델을 훈련할 때마다 증가시킴\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45dec7",
   "metadata": {},
   "source": [
    "이 모델은 검증 세트에서 48.9% 정확도에 도달합니다. 드롭아웃이 없을 때보다(47.6%) 조금 더 좋습니다. 하이퍼파라미터 탐색을 좀 많이 수행해 보면 더 나아 질 수 있습니다(드롭아웃 비율 5%, 10%, 20%, 40%과 학습률 1e-4, 3e-4, 5e-4, 1e-3을 시도했습니다). 하지만 이 경우에는 크지 않을 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c48c6",
   "metadata": {},
   "source": [
    "이제 MC 드롭아웃을 사용해 보죠. 앞서 사용한 MCAlphaDropout 클래스를 복사해 사용하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cee976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2187c7f",
   "metadata": {},
   "source": [
    "방금 훈련했던 모델과 (같은 가중치를 가진) 동일한 새로운 모델을 만들어 보죠. 하지만 AlphaDropout 층 대신 MCAlphaDropout 드롭아웃 층을 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b512b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda24bc",
   "metadata": {},
   "source": [
    "그다음 몇 가지 유틸리티 함수를 추가합니다. 첫 번째 함수는 모델을 여러 번 실행합니다(기본적으로 10번). 그다음 평균한 예측 클래스 확률을 반환합니다. 두 번째 함수는 이 평균 확률을 사용해 각 샘플의 클래스를 예측합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7eae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d6bade8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ba665",
   "metadata": {},
   "source": [
    "이 경우에는 정확도 향상이 없습니다(여전히 정확도는 48.9%입니다).\n",
    "\n",
    "따라서 이 연습문에서 얻은 최상의 모델은 배치 정규화 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4963608",
   "metadata": {},
   "source": [
    "f.문제: 1사이클 스케줄링으로 모델을 다시 훈련하고 훈련 속도와 모델 정확도가 향상되는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a20f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3718ae",
   "metadata": {},
   "source": [
    "#1사이클 스케쥴링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7293358",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e726f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 3s 7ms/step - loss: nan - accuracy: 0.1381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.626567840576172,\n",
       " 3.942450114658901)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7254ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3635e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dc826ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 2.0575 - accuracy: 0.2833 - val_loss: 1.7801 - val_accuracy: 0.3766\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.7558 - accuracy: 0.3804 - val_loss: 1.6452 - val_accuracy: 0.4196\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.6187 - accuracy: 0.4235 - val_loss: 1.6110 - val_accuracy: 0.4328\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.5416 - accuracy: 0.4511 - val_loss: 1.6320 - val_accuracy: 0.4248\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.4885 - accuracy: 0.4716 - val_loss: 1.5921 - val_accuracy: 0.4574\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.4458 - accuracy: 0.4878 - val_loss: 1.5663 - val_accuracy: 0.4612\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 3s 7ms/step - loss: 1.4067 - accuracy: 0.5027 - val_loss: 1.5254 - val_accuracy: 0.4644\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.3391 - accuracy: 0.5252 - val_loss: 1.5021 - val_accuracy: 0.4882\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.2651 - accuracy: 0.5514 - val_loss: 1.5236 - val_accuracy: 0.4704\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.1942 - accuracy: 0.5742 - val_loss: 1.5210 - val_accuracy: 0.4916\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 3s 7ms/step - loss: 1.1258 - accuracy: 0.5988 - val_loss: 1.5360 - val_accuracy: 0.5080\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.0576 - accuracy: 0.6238 - val_loss: 1.4855 - val_accuracy: 0.5064\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 0.9865 - accuracy: 0.6477 - val_loss: 1.5215 - val_accuracy: 0.5222\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 3s 7ms/step - loss: 0.9248 - accuracy: 0.6711 - val_loss: 1.5397 - val_accuracy: 0.5224\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 0.8843 - accuracy: 0.6853 - val_loss: 1.5685 - val_accuracy: 0.5224\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e76db",
   "metadata": {},
   "source": [
    "1사이클 방식을 사용해 모델을 15에포크 동안 훈련했습니다. (큰 배치 크기 덕분에) 각 에포크는 2초만 걸렸습니다. 이는 지금까지 훈련한 가장 빠른 모델보다 몇 배 더 빠릅니다. 또한 모델 성능도 올라갔습니다(47.6%에서 52.0%). 배치 정규화 모델이 조금 더 성능(54%)이 높지만 훈련 속도가 더 느립니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
